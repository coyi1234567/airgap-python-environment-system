# æ¨¡å‹ç®¡ç†è„šæœ¬ - ç®¡ç†AIæ¨¡å‹çš„ä¸‹è½½ã€é…ç½®å’Œéƒ¨ç½²
# Model Manager Script - Manage AI model download, configuration and deployment

param(
    [string]$Action = "list",
    [string]$ModelType = "",
    [string]$ModelName = "",
    [string]$Quantization = "",
    [string]$InferenceEngine = "transformers",
    [string]$ConfigFile = "config/model-config.json"
)

function Write-Log {
    param(
        [string]$Message,
        [string]$Level = "INFO"
    )
    
    $timestamp = Get-Date -Format "yyyy-MM-dd HH:mm:ss"
    $logMessage = "[$timestamp] [$Level] $Message"
    
    switch ($Level) {
        "ERROR" { Write-Host $logMessage -ForegroundColor Red }
        "WARNING" { Write-Host $logMessage -ForegroundColor Yellow }
        "SUCCESS" { Write-Host $logMessage -ForegroundColor Green }
        default { Write-Host $logMessage -ForegroundColor White }
    }
}

function Get-ModelConfig {
    param([string]$ConfigFile)
    
    if (-not (Test-Path $ConfigFile)) {
        Write-Log "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $ConfigFile" "ERROR"
        return $null
    }
    
    try {
        $config = Get-Content $ConfigFile | ConvertFrom-Json
        return $config
    } catch {
        Write-Log "é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: $($_.Exception.Message)" "ERROR"
        return $null
    }
}

function Show-ModelList {
    param([object]$Config)
    
    Write-Log "=== å¯ç”¨æ¨¡å‹åˆ—è¡¨ ===" "INFO"
    
    foreach ($modelType in $Config.model_configurations.PSObject.Properties) {
        Write-Host "`nğŸ“¦ $($modelType.Name.ToUpper()) æ¨¡å‹:" -ForegroundColor Cyan
        
        foreach ($model in $modelType.Value.models.PSObject.Properties) {
            $modelInfo = $model.Value
            Write-Host "  ğŸ”¹ $($model.Name)" -ForegroundColor Yellow
            Write-Host "     ID: $($modelInfo.model_id)" -ForegroundColor Gray
            Write-Host "     å¤§å°: $($modelInfo.size_gb) GB" -ForegroundColor Gray
            Write-Host "     é‡åŒ–: $($modelInfo.quantization -join ', ')" -ForegroundColor Gray
            Write-Host "     æ¨ç†å¼•æ“: $($modelInfo.inference_engines -join ', ')" -ForegroundColor Gray
        }
    }
}

function Show-InferenceEngines {
    param([object]$Config)
    
    Write-Log "=== æ¨ç†å¼•æ“åˆ—è¡¨ ===" "INFO"
    
    foreach ($engine in $Config.inference_configurations.PSObject.Properties) {
        $engineInfo = $engine.Value
        Write-Host "`nğŸ”§ $($engine.Name.ToUpper())" -ForegroundColor Cyan
        Write-Host "   æè¿°: $($engineInfo.description)" -ForegroundColor White
        Write-Host "   é‡åŒ–æ”¯æŒ: $($engineInfo.quantization_support)" -ForegroundColor Gray
        Write-Host "   GPUåŠ é€Ÿ: $($engineInfo.gpu_acceleration)" -ForegroundColor Gray
        if ($engineInfo.batch_inference) {
            Write-Host "   æ‰¹é‡æ¨ç†: $($engineInfo.batch_inference)" -ForegroundColor Gray
        }
    }
}

function Show-QuantizationOptions {
    param([object]$Config)
    
    Write-Log "=== é‡åŒ–é€‰é¡¹ ===" "INFO"
    
    foreach ($quant in $Config.quantization_configurations.PSObject.Properties) {
        $quantInfo = $quant.Value
        Write-Host "`nâš¡ $($quant.Name.ToUpper())" -ForegroundColor Cyan
        Write-Host "   æè¿°: $($quantInfo.description)" -ForegroundColor White
        Write-Host "   å†…å­˜å‡å°‘: $($quantInfo.memory_reduction)" -ForegroundColor Gray
        Write-Host "   æ€§èƒ½å½±å“: $($quantInfo.performance_impact)" -ForegroundColor Gray
    }
}

function Get-ModelRequirements {
    param(
        [object]$Config,
        [string]$ModelType,
        [string]$ModelName
    )
    
    if (-not $Config.model_configurations.$ModelType) {
        Write-Log "æ¨¡å‹ç±»å‹ä¸å­˜åœ¨: $ModelType" "ERROR"
        return $null
    }
    
    if (-not $Config.model_configurations.$ModelType.models.$ModelName) {
        Write-Log "æ¨¡å‹ä¸å­˜åœ¨: $ModelName" "ERROR"
        return $null
    }
    
    $model = $Config.model_configurations.$ModelType.models.$ModelName
    return $model
}

function Test-ModelCompatibility {
    param(
        [object]$Model,
        [string]$Quantization,
        [string]$InferenceEngine
    )
    
    $compatible = $true
    $issues = @()
    
    # æ£€æŸ¥é‡åŒ–å…¼å®¹æ€§
    if ($Quantization -and $Model.quantization -notcontains $Quantization) {
        $compatible = $false
        $issues += "æ¨¡å‹ä¸æ”¯æŒ $Quantization é‡åŒ–"
    }
    
    # æ£€æŸ¥æ¨ç†å¼•æ“å…¼å®¹æ€§
    if ($InferenceEngine -and $Model.inference_engines -notcontains $InferenceEngine) {
        $compatible = $false
        $issues += "æ¨¡å‹ä¸æ”¯æŒ $InferenceEngine æ¨ç†å¼•æ“"
    }
    
    return @{
        Compatible = $compatible
        Issues = $issues
    }
}

function Generate-ModelScript {
    param(
        [object]$Model,
        [string]$ModelType,
        [string]$ModelName,
        [string]$Quantization,
        [string]$InferenceEngine
    )
    
    $scriptContent = @"
# æ¨¡å‹åŠ è½½è„šæœ¬ - $ModelName
# Generated by Model Manager

import torch
from transformers import AutoTokenizer, AutoModel

def load_model():
    """åŠ è½½ $ModelName æ¨¡å‹"""
    
    model_id = "$($Model.model_id)"
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    
    # åŠ è½½æ¨¡å‹
    model = AutoModel.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    return model, tokenizer

def chat(model, tokenizer, query, history=None):
    """ä¸æ¨¡å‹å¯¹è¯"""
    
    if history is None:
        history = []
    
    response, history = model.chat(tokenizer, query, history=history)
    return response, history

if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model, tokenizer = load_model()
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    # äº¤äº’å¼å¯¹è¯
    history = []
    while True:
        query = input("ç”¨æˆ·: ")
        if query.lower() in ['quit', 'exit', 'é€€å‡º']:
            break
        
        response, history = chat(model, tokenizer, query, history)
        print(f"åŠ©æ‰‹: {response}")
"@
    
    $scriptPath = "scripts/model-$ModelName.py"
    Set-Content -Path $scriptPath -Value $scriptContent -Encoding UTF8
    Write-Log "æ¨¡å‹è„šæœ¬å·²ç”Ÿæˆ: $scriptPath" "SUCCESS"
}

function Start-ModelManager {
    Write-Log "=== AIæ¨¡å‹ç®¡ç†å™¨ ===" "INFO"
    Write-Log "æ“ä½œ: $Action" "INFO"
    Write-Log "æ¨¡å‹ç±»å‹: $ModelType" "INFO"
    Write-Log "æ¨¡å‹åç§°: $ModelName" "INFO"
    Write-Log "é‡åŒ–: $Quantization" "INFO"
    Write-Log "æ¨ç†å¼•æ“: $InferenceEngine" "INFO"
    
    # åŠ è½½é…ç½®
    $config = Get-ModelConfig -ConfigFile $ConfigFile
    if (-not $config) {
        return $false
    }
    
    switch ($Action.ToLower()) {
        "list" {
            Show-ModelList -Config $config
            Show-InferenceEngines -Config $config
            Show-QuantizationOptions -Config $config
        }
        
        "info" {
            if (-not $ModelType -or -not $ModelName) {
                Write-Log "è¯·æŒ‡å®šæ¨¡å‹ç±»å‹å’Œæ¨¡å‹åç§°" "ERROR"
                return $false
            }
            
            $model = Get-ModelRequirements -Config $config -ModelType $ModelType -ModelName $ModelName
            if ($model) {
                Write-Log "=== æ¨¡å‹ä¿¡æ¯ ===" "INFO"
                Write-Host "æ¨¡å‹ID: $($model.model_id)" -ForegroundColor White
                Write-Host "æ¨¡å‹ç±»å‹: $($model.model_type)" -ForegroundColor White
                Write-Host "å¤§å°: $($model.size_gb) GB" -ForegroundColor White
                Write-Host "é‡åŒ–é€‰é¡¹: $($model.quantization -join ', ')" -ForegroundColor White
                Write-Host "æ¨ç†å¼•æ“: $($model.inference_engines -join ', ')" -ForegroundColor White
                Write-Host "ä¾èµ–åŒ…: $($model.requirements | ConvertTo-Json -Compress)" -ForegroundColor White
            }
        }
        
        "check" {
            if (-not $ModelType -or -not $ModelName) {
                Write-Log "è¯·æŒ‡å®šæ¨¡å‹ç±»å‹å’Œæ¨¡å‹åç§°" "ERROR"
                return $false
            }
            
            $model = Get-ModelRequirements -Config $config -ModelType $ModelType -ModelName $ModelName
            if ($model) {
                $compatibility = Test-ModelCompatibility -Model $model -Quantization $Quantization -InferenceEngine $InferenceEngine
                
                if ($compatibility.Compatible) {
                    Write-Log "æ¨¡å‹é…ç½®å…¼å®¹" "SUCCESS"
                } else {
                    Write-Log "æ¨¡å‹é…ç½®ä¸å…¼å®¹" "ERROR"
                    foreach ($issue in $compatibility.Issues) {
                        Write-Log "  - $issue" "ERROR"
                    }
                }
            }
        }
        
        "generate" {
            if (-not $ModelType -or -not $ModelName) {
                Write-Log "è¯·æŒ‡å®šæ¨¡å‹ç±»å‹å’Œæ¨¡å‹åç§°" "ERROR"
                return $false
            }
            
            $model = Get-ModelRequirements -Config $config -ModelType $ModelType -ModelName $ModelName
            if ($model) {
                $compatibility = Test-ModelCompatibility -Model $model -Quantization $Quantization -InferenceEngine $InferenceEngine
                
                if ($compatibility.Compatible) {
                    Generate-ModelScript -Model $model -ModelType $ModelType -ModelName $ModelName -Quantization $Quantization -InferenceEngine $InferenceEngine
                } else {
                    Write-Log "æ¨¡å‹é…ç½®ä¸å…¼å®¹ï¼Œæ— æ³•ç”Ÿæˆè„šæœ¬" "ERROR"
                }
            }
        }
        
        default {
            Write-Log "æœªçŸ¥æ“ä½œ: $Action" "ERROR"
            Write-Log "æ”¯æŒçš„æ“ä½œ: list, info, check, generate" "INFO"
        }
    }
    
    return $true
}

# ä¸»æ‰§è¡Œé€»è¾‘
if ($MyInvocation.InvocationName -ne '.') {
    Start-ModelManager
}
